{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Getting started with Deep Learning, CNN & PyTorch",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "organizations_zalando_research_fashionmnist_path = kagglehub.dataset_download('organizations/zalando-research/fashionmnist')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "eFbDrSG57uPS"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started with Deep Learning, CNN & PyTorch\n",
        "\n",
        "In this Notebook I will use the **PyTorch** tensor library to classify the Fashion MNIST dataset using Convolutional Neural Networks.   \n",
        "\n",
        "I will also take some time to introduce all concepts and tools I'm using here. Feel free to skip if you want to get straight to the code. I added the table of contents below to add you navigate easily.\n",
        "\n",
        "[1. Introduction](#1.-Introduction)  \n",
        "[2. Data exploration](#1.-Data-exploration)  \n",
        "[3. Creating our own Convolutional Neural Network model](#3.-Creating-our-own-Convolutional-Neural-Network-model)  \n",
        "[4. Training the CNN model](#4.-Training-the-CNN-model)  \n",
        "[5. CNN model evaluation](#5.-CNN-model-evaluation)"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "id": "Sqkf2nob7uPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "\n",
        "# What is PyTorch\n",
        "\n",
        "\n",
        "PyTorch is an open source machine learning library. It's mainly used for **computer vision** and **natural language** processing. PyTorch was developed by Facebook and initially released in 2016. If this sparks your interest to learn more about its past and present, I invite you to explore <a href='https://en.wikipedia.org/wiki/PyTorch'>PyTorch's Wikipedia page</a>.\n",
        "\n",
        "One more thing you might like to know is that Tesla Autopilot is built on top of PyTorch. And if you are a fan of Tesla, you might enjoy this talk where <a href='https://en.wikipedia.org/wiki/PyTorch'>Andrej Karpathy discusses how Tesla is using PyTorch</a>.\n",
        "\n",
        "## How do I learn PyTorch\n",
        "\n",
        "The answer is, of course, through building something. <a href='https://www.linkedin.com/in/brohrer'>Brandon Rohrer</a> has a nice introductory blog post on <a href='https://e2eml.school/one_step_program_become_data_scientist.html'>becoming a Data Scientist</a> and his \"how to\" recipe is exactly this: \"build stuff\". He even has a guide for <a href='https://e2eml.school/choosing_project.html'>choosing a data science project to build.</a>\n",
        "\n",
        "## Where do I learn PyTorch from\n",
        "\n",
        "From my experience - and I also found this <a href='https://www.reddit.com/r/MachineLearning/comments/9rvxj6/d_best_bookcourse_to_get_started_with_pytorch/'>Reddit discussion</a> to confirm my beliefs (confirmation bias), the official PyTorch tutorials on https://pytorch.org/ are a great learning resource. If the official website is not to your liking, explore the Reddit discussion and you will find more resources.\n",
        "\n",
        "## PyTorch or TensorFlow\n",
        "\n",
        "You probably heard about TensorFlow. Even if you don't know exactly what it is, it probably rings a bell. Well, it looks like PyTorch and TensorFlow do the same job.\n",
        "\n",
        "TensorFlow was developed by Google and it was publicly released one year before PyTorch. But since PyTorch appeared, TensorFlow started to decrease in popularity.\n",
        "\n",
        "PyTorch has the reputation of being used predominantly in the academic environment, in research, while TensorFlow was known to be used in production. Things have been changing though, with companies like Tesla and Uber adopting PyTorch.\n",
        "\n",
        "Check out this article for an ample discussion on <a href='https://realpython.com/pytorch-vs-tensorflow/#pytorch-vs-tensorflow-decision-guide'>PyTorch versus TensorFlow.</a>\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "Everybody is talking about neural networks (NN) these days. And about Deep Learning, which is a particular type of NN.\n",
        "If you've heard these terms many times, don't feel like you know exactly what they refer to and don't want to read one whole book just to clarify the concepts, here is a quick overview:\n",
        "\n",
        "Artificial Neural Networks (ANN) and Neural Networks (NN) are the same thing. People use NN because it's shorter than ANN.\n",
        "\n",
        "I got a good introduction to the topic through the course of Jose Portilla on Udemy - \"Python for Data Science and Machine Learning Bootcamp\". Section 25 is dedicated to NN. It has a little bit of theory and a lot of practice (though it uses TensorFlow for the practical work). The whole section takes about 5 hours of videos (theory and code along short videos).\n",
        "\n",
        "If you want to go deeper and understand the Math behind NNs, I used <a href='https://www.linkedin.com/in/andriyburkov/?originalSubdomain=ca'>Andriy Burkov's</a> the <a href='https://www.dropbox.com/s/uh48e6wjs4w13t5/Chapter6.pdf?dl=0'>Hundred-Page Machine Learning Book</a>. Chapter 6 takes you through 17 pages of Neural Networks quick overview. //The author makes the book available on a read first pay later principle. So, if you enjoy the book, you can purchase at the end using any of <a href='http://themlbook.com/'>the methods here.</a>\n",
        "\n",
        "And if you want to understand exactly how neural nets work and work your way through the backpropagation with pen and paper, read <a href='https://en.wikipedia.org/wiki/Michael_Nielsen'>Michael Nielsen's</a> <a href='http://neuralnetworksanddeeplearning.com/index.html'>Neural Networks and Deep Learning: Introduction to the core principles.</a>\n",
        "\n",
        "## Convolutional Neural Networks\n",
        "\n",
        "Convolutional Neural Networks (CNN) are a subclass of Neural Networks that solve some of the shortcomings of the usual NNs. One main advantage is the reduction of parameters that need to be set through training the model.\n",
        "\n",
        "For a full explanation of the benefits of CNN over NN, I recommend section 8 of the course \"Python for Data Science and Machine Learning Bootcamp\" (by Jose Portilla, on Udemy). This section takes around 5 hours to complete, contains theoetical and code along videos and takes you through examples where you use NN and CNN for the same data."
      ],
      "metadata": {
        "id": "8VnzV-hj7uPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fashion MNIST CNN with PyTorch\n",
        "\n",
        "## The MNIST dataset\n",
        "\n",
        "The Fashion MNIST dataset consists of 60.000 training images and 10.000 test images.   \n",
        "The examples are 28x28 pixels, greyscale images of clothing items (as the data comes from Zalando, an e-commerce fashion reseller).\n",
        "Each item belongs to one of the following categories:    \n",
        "\n",
        "0. T-shirt/top\n",
        "1. Trouser\n",
        "2. Pullover\n",
        "3. Dress\n",
        "4. Coat\n",
        "5. Sandal\n",
        "6. Shirt\n",
        "7. Sneaker\n",
        "8. Bag\n",
        "9. Ankle boot\n",
        "\n",
        "For a complete description of the MNIST dataset, please check <a href='https://www.kaggle.com/zalando-research/fashionmnist'>this page on Kaggle</a>. Check the tab labelled 'Data' and you will see a very detailed description."
      ],
      "metadata": {
        "id": "Qsj0hJBu7uPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Imports\n",
        "\n",
        "We begin by importing the usual libraries for a PyTorch project."
      ],
      "metadata": {
        "id": "kQEXAOT37uPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import time"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:18.694351Z",
          "iopub.execute_input": "2022-01-27T16:35:18.694704Z",
          "iopub.status.idle": "2022-01-27T16:35:20.717816Z",
          "shell.execute_reply.started": "2022-01-27T16:35:18.694674Z",
          "shell.execute_reply": "2022-01-27T16:35:20.717073Z"
        },
        "trusted": true,
        "id": "fW8icEpT7uPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data exploration"
      ],
      "metadata": {
        "id": "EFl24rER7uPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Reading and examining our data  \n",
        "We will first use the easy way to read our data: we know we have the data in csv files. The easiest way I know for reading a CSV is to use the read_csv function from pandas. This will read the contents of each file into a pandas DataFrame.  \n",
        "\n",
        "Later we will see that we're better off using the <a href='https://pytorch.org/docs/stable/data.html'>DataLoader</a> class from the PyTorch library. And I will explain why. But, for now, let us stick to the familiar pandas DataFrame just to see what kind of data we are dealing with a how it is organised."
      ],
      "metadata": {
        "id": "AxxzzQdu7uPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the training and test data into pandas DataFrames."
      ],
      "metadata": {
        "id": "eFyQ6wuk7uPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv',sep=',')\n",
        "test_data = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv', sep = ',')\n",
        "\n",
        "class_names = ['T-shirt','Trouser','Sweater','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Boot']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:20.719542Z",
          "iopub.execute_input": "2022-01-27T16:35:20.719865Z",
          "iopub.status.idle": "2022-01-27T16:35:27.132056Z",
          "shell.execute_reply.started": "2022-01-27T16:35:20.719837Z",
          "shell.execute_reply": "2022-01-27T16:35:27.131224Z"
        },
        "trusted": true,
        "id": "Ls7AAECh7uPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore our data.   \n",
        "Since we read our data as a pandas DataFrame, we'll use the usual head() method."
      ],
      "metadata": {
        "id": "aIsJHXYn7uP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train_data dimensions: \", train_data.shape)\n",
        "train_data.head(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:27.135264Z",
          "iopub.execute_input": "2022-01-27T16:35:27.135567Z",
          "iopub.status.idle": "2022-01-27T16:35:27.163339Z",
          "shell.execute_reply.started": "2022-01-27T16:35:27.135539Z",
          "shell.execute_reply": "2022-01-27T16:35:27.16245Z"
        },
        "trusted": true,
        "id": "-y2ftmhA7uP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we are dealing with 60.000 rows x 785 columns.   \n",
        "That's because we have 60.000 training examples.   \n",
        "\n",
        "But why is the row length equal to 785 ? Look at the column names in the table above. It looks like our images are flattened out into a long one-dimensional array. We know our images are 28x28 pixels. If you flatten one image out (collapse into one dimension instead of two), it gives us one row of length 784.      \n",
        "And the first column represents the label of that image (one digit, from 0 to 9, for each of the 10 clothing item types we have in this dataset).  \n",
        "These add up to the 785 which is our row's length."
      ],
      "metadata": {
        "id": "Vo7auEYd7uP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How large is our test set ?"
      ],
      "metadata": {
        "id": "4UkRRw1N7uP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train_data dimensions: \", test_data.shape)\n",
        "test_data.head(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:27.164975Z",
          "iopub.execute_input": "2022-01-27T16:35:27.165325Z",
          "iopub.status.idle": "2022-01-27T16:35:27.183473Z",
          "shell.execute_reply.started": "2022-01-27T16:35:27.165289Z",
          "shell.execute_reply": "2022-01-27T16:35:27.182635Z"
        },
        "trusted": true,
        "id": "nGw5Z-g07uP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Examine one image"
      ],
      "metadata": {
        "id": "5Ld3ZVER7uP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's look at the first example in our training dataset\n",
        "label = train_data.iloc[0,0]\n",
        "image = train_data.iloc[0,1:]\n",
        "print('Shape:', image.shape, '\\nLabel:', label, '\\nClass:', class_names[label])\n",
        "\n",
        "torch_tensor = torch.tensor(image.values)\n",
        "plt.figure(figsize=(1,1))\n",
        "plt.imshow(torch_tensor.reshape((28,28)), cmap=\"gray\");"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:27.186954Z",
          "iopub.execute_input": "2022-01-27T16:35:27.187292Z",
          "iopub.status.idle": "2022-01-27T16:35:27.322972Z",
          "shell.execute_reply.started": "2022-01-27T16:35:27.187258Z",
          "shell.execute_reply": "2022-01-27T16:35:27.321988Z"
        },
        "trusted": true,
        "id": "18BEHsBs7uP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 PyTorch DataLoader\n",
        "\n",
        "Remember I mentioned in section 2.1 that we're only using pandas to explore the data temporarily and that later we will look at the tools PyTorch offers for loading data. The time has come.\n",
        "\n",
        "### Loading data in batches\n",
        "Usually, Data Scientists handle huge amounts of data. You may think that 60.000 training examples is not that much and you're right. But this is just our playground now. And we are preparing for real world data analysis, so it's good to create good habits from the start. Besides having huge amounts of data to read, we also have a large number of parameters to train in our CNN model. We will compute the exact number of parameter later. For the time being, keep in mind that we will be training around 60.000 network parameters.   \n",
        "\n",
        "The bottom line is that we want to save memory space. For this purpose, PyTorch offers <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\">DataLoader</a>, which allows us to load data in batches. That is, instead of loading all 60.000 examples in memory at the same time, we will load {batch_size} examples at a time. I will use a batch_size of 10 just because I've seen it used so many times, but feel free to experiment.\n",
        "\n",
        "### Custom Dataset\n",
        "The common way to use DataLoader is like this:\n",
        "\n",
        "> from torchvision import datasets, transforms   \n",
        "> from torch.utils.data import DataLoader  \n",
        "> transform = transforms.ToTensor()   \n",
        "> train_data = datasets.FashionMNIST(root='/kaggle/input/', train=True, download=True, transform=transform)  \n",
        "> train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "\n",
        "This method uses <a href='https://pytorch.org/docs/stable/torchvision/index.html'><tt><strong>torchvision</strong></tt></a> to download an MNIST dataset. On the first call, the dataset is downloaded in the indicated folder. Subsequent calls will check if the data is already downloaded and will use the local version.   \n",
        "\n",
        "We need to transform our dataset (the csv files) into the kind of data that DataLoader expects.\n",
        "\n",
        "Check what kind of data DataLoader expects:\n",
        "- <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\">official Python documentation for DataLoader.</a>\n",
        "- looks like we need to convert our data to a 'Dataset'. Check <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\">PyTorch documentation for Dataset</a>.\n",
        "- Dataset is an abstract class (more appropriately called an \"interface\") --> we need to implement our own class to extend it.\n",
        "- According to the documentation, our class must overwrite __getitem__() and it should (optionally) overwrite __getlen__()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZWfplvQ37uP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        data = pd.read_csv(file_path,sep=',')\n",
        "\n",
        "        # convert the pandas DataFrames to torch tensors\n",
        "        # separate tensors for labels and for features (our image pixels)\n",
        "        self.labels = torch.tensor(data['label'].values)\n",
        "\n",
        "        images = torch.tensor(data.drop(columns=['label']).values)\n",
        "        self.images = torch.reshape(images,(-1, 1, 28, 28))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx],self.labels[idx]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:27.325727Z",
          "iopub.execute_input": "2022-01-27T16:35:27.326077Z",
          "iopub.status.idle": "2022-01-27T16:35:27.333496Z",
          "shell.execute_reply.started": "2022-01-27T16:35:27.326041Z",
          "shell.execute_reply": "2022-01-27T16:35:27.332507Z"
        },
        "trusted": true,
        "id": "ejbLihTp7uP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = FashionMNISTDataset('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\n",
        "train_loader = DataLoader(train_data, batch_size = 10, shuffle = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:27.334714Z",
          "iopub.execute_input": "2022-01-27T16:35:27.335194Z",
          "iopub.status.idle": "2022-01-27T16:35:31.616212Z",
          "shell.execute_reply.started": "2022-01-27T16:35:27.335159Z",
          "shell.execute_reply": "2022-01-27T16:35:31.615436Z"
        },
        "trusted": true,
        "id": "6YYL0eme7uP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few observations on the code above:\n",
        "- It might not be necessary now, but it's a good practice to always shuffle the training data. Some datasets may have all the exemplars of a class grouped together (e.g. all the t-shirt images, then all the shoes images etc). We don't want our CNN to train on a single class, then on another class etc.\n",
        "- I use batch size 10 just because it seems to be fashionable, no other reasons for this particular number for me."
      ],
      "metadata": {
        "id": "jQBOMYlR7uP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check our custom DataLoader. We'll take the first batch of 10 examples and print them as images. Let's see if the image looks ok and if it matched its associated label."
      ],
      "metadata": {
        "id": "4CqfmDCG7uP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images,labels in train_loader:\n",
        "    break\n",
        "\n",
        "#check our data format\n",
        "print(\"Shape of our chunk of images: \",images.shape)\n",
        "\n",
        "# Print the labels\n",
        "print('Labels:', labels.numpy())\n",
        "print('Classes:', *np.array([class_names[i] for i in labels]))\n",
        "\n",
        "img = make_grid(images, nrow=10)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(np.transpose(img.numpy(), (1, 2, 0))); #we need to transpose our images because plt.imshow expects another shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:31.617788Z",
          "iopub.execute_input": "2022-01-27T16:35:31.618151Z",
          "iopub.status.idle": "2022-01-27T16:35:31.781234Z",
          "shell.execute_reply.started": "2022-01-27T16:35:31.618113Z",
          "shell.execute_reply": "2022-01-27T16:35:31.780349Z"
        },
        "trusted": true,
        "id": "WGRufYd77uP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good, our custom DataLoder seems to work fine.   \n",
        "Let's load the test data too."
      ],
      "metadata": {
        "id": "tkdQM2nO7uQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = FashionMNISTDataset('/kaggle/input/fashionmnist/fashion-mnist_test.csv')\n",
        "test_loader = DataLoader(test_data, batch_size = 10, shuffle = False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:31.782969Z",
          "iopub.execute_input": "2022-01-27T16:35:31.783365Z",
          "iopub.status.idle": "2022-01-27T16:35:32.620572Z",
          "shell.execute_reply.started": "2022-01-27T16:35:31.783322Z",
          "shell.execute_reply": "2022-01-27T16:35:32.619396Z"
        },
        "trusted": true,
        "id": "nrkndvz_7uQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Creating our own Convolutional Neural Network model\n",
        "\n",
        "To create our own CNN in PyTorch, we need to define our own class as a subclass of __Module__. If you feel the need to read more about this procedure, check out the <a href=\"https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html\">Custom nn Modules examples in the official PyTorch documentation</a>.\n",
        "\n",
        "## 3.1 The structure of a CNN\n",
        "\n",
        "Regular NN models would conain fully connected layers with flat inputs (1 dimensional).  \n",
        "Let's a little talk about what is specific to CNN and not found in regular NNs in general.\n",
        "CNNs use:\n",
        "- convolutional layers made up of image filters which are based on image kernels\n",
        "    - to see image kernels in action, check out this <a href=\"https://setosa.io/ev/image-kernels/\">visual explanation of image kernels </a>\n",
        "- pooling layers, which are used to reduce the dimensionality of convolutional layers outputs.\n",
        "\n",
        "## 3.2 Defining our CNN model\n",
        "\n",
        "In our CNN model, we will implement this architecture:\n",
        "1. convolutional layer 1 (1,6,3,1) ->\n",
        "2. pooling layer 1 (2, 2) ->\n",
        "3. convolutional  layer 2 (6,16,3,1) ->\n",
        "4. pooling layer 2 (2,2) ->\n",
        "5. fully connected layer 1 or fc1 (5x5x16, 120) ->\n",
        "6. fully connected layer 2 or fc2 (120,84) ->\n",
        "7. fully connected layer 3 or fc3 (84,10), which is our output layer.\n",
        "\n",
        "The data in parathesis represent the parameters of each layer and here is an explanation:\n",
        "1. convolutional layer 1 has parameters set to (1,6,3,1) because it has:\n",
        "    - 1 input channel (because our data is greyscale, so we only have one color channel)\n",
        "    - 6 output channels (these are esentially the number of filters I arbitrarily chose for this layer)\n",
        "    - 3 is the kernel size (in effect, this means a 3x3 kernel)\n",
        "    - 1 is the step size\n",
        "2. pooling layer 1 has a kernel of size 2 (2x2) and a step of 2 (also referred to as 'stride')\n",
        "3. convolutional layer 2 will have: 6 input channels (because conv1 was outputting 6 channels and we pull them into conv2), 16 output channels (16 filters), a kernel size of 3 (again, this means 3x3) and a step size of 1\n",
        "4. pooling layer 2 also has a 2x2 kernel and a stride of 2\n",
        "5. fc1 has:\n",
        "    - an input of size 5x5x16. The 16 comes from the number of channels we chose for the previous layer, conv2. The 5x5 comes from the dimensionality reduction that is created through the use of the previous layers. By using 2 convolutional layers and 2 pooling layers, we go from 28x28 input data to 5x5 matrices.\n",
        "    - and the layer has 100 neurons (I arbitrarily chose this number)\n",
        "6. I bet you know by now why fc2 is (120,84)\n",
        "7. and you also know why fc3 has these settings\n",
        "\n",
        "*if in doubt whether I selected 120 and 84 numbers of neurons in the 1st and 2nd fully connected layers for some reason, know that I did not. These are just numbers I picked out. How to decide on the dimensions of neural networks layers is beyond the scope of this tutorial.\n"
      ],
      "metadata": {
        "id": "aKeuFHRM7uQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1,6,3,1)\n",
        "        self.conv2 = nn.Conv2d(6,16,3,1)\n",
        "        self.fc1 = nn.Linear(5*5*16,120)\n",
        "        self.fc2 = nn.Linear(120,84)\n",
        "        self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = F.relu(self.conv1(X))\n",
        "        X = F.max_pool2d(X,2,2)\n",
        "        X = F.relu(self.conv2(X))\n",
        "        X = F.max_pool2d(X,2,2)\n",
        "        X = X.view(-1, 5*5*16) # we need to flatten our data to 1D before inputting it to our fully connected layers.\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = F.relu(self.fc2(X))\n",
        "        return F.log_softmax(X, dim=1)\n",
        "\n",
        "torch.manual_seed(101)\n",
        "model = myCNN()\n",
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:32.625811Z",
          "iopub.execute_input": "2022-01-27T16:35:32.628248Z",
          "iopub.status.idle": "2022-01-27T16:35:32.658535Z",
          "shell.execute_reply.started": "2022-01-27T16:35:32.628206Z",
          "shell.execute_reply": "2022-01-27T16:35:32.65765Z"
        },
        "trusted": true,
        "id": "ve9KKG3x7uQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some clarifications on the code above:\n",
        "- the output of the convolutional layers is passed through a Rectified Linear Unit activation function. If you need to read more about <a href=\"https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning\">why and how the ReLU function is used in NNs, there is a short Notebook here on Kaggle.</a>\n",
        "- for the pooling layers, I choose a max 2D pooling function and you can <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d\">read more about it here.</a>\n",
        "- for the output layer, the log soft max is the most commonly used function for multiclass classification situations.\n"
      ],
      "metadata": {
        "id": "8F0yC6mi7uQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Parameters downsize with CNN\n",
        "\n",
        "I mentioned earlier that one advantage of CNN over regular NN is the decrease in the number of parameters that need to be trained.\n",
        "\n",
        "Let's see if it's true. If we used a regular NN with the same fullyconnected layers: fc1(120 neurons), fc2(84 neurons) and fc3(10 neurons), we would need to train this numer of parameters:\n",
        "+ 784 (the input size = 28\\*28 images) * 120 neurons = 94.080 weights for fc1\n",
        "+ 120 biases for fc1\n",
        "+ 120 * 84 = 10.080 weights for fc2\n",
        "+ 120 biases for fc2\n",
        "+ 84 * 10 = 840 weights for fc3\n",
        "+ 10 biases for fc3\n",
        "------------\n",
        "= 105.250 total number of parameters to train\n",
        "\n",
        "And how many parameters do we need to train in our CNN ? The following code will compute the number:"
      ],
      "metadata": {
        "id": "ty2n6U587uQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def param_count(model):\n",
        "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
        "    for item in params:\n",
        "        print(item)\n",
        "    print('Total: ', sum(params))\n",
        "\n",
        "param_count(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:32.662383Z",
          "iopub.execute_input": "2022-01-27T16:35:32.664664Z",
          "iopub.status.idle": "2022-01-27T16:35:32.677435Z",
          "shell.execute_reply.started": "2022-01-27T16:35:32.664584Z",
          "shell.execute_reply": "2022-01-27T16:35:32.676465Z"
        },
        "trusted": true,
        "id": "zgvbUBx-7uQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Therefore, by using our CNN instead of an ANN with the same fully connected layers, we have to train 60.074 parameters instead of 105.250**"
      ],
      "metadata": {
        "id": "BzceO92z7uQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training the CNN model\n",
        "\n",
        "Before starting to train our model, we need to define a loss function and an optimizer.\n",
        "\n",
        "Since we are dealing with a problem of classification, I will use the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\">nn.CrossEntropyLoss</a> function for the criterion on which to evaluate how well the model is performing.\n",
        "\n",
        "For the optimizer, I will use Adam, which is the most widely used algorithm for training deep learning networks."
      ],
      "metadata": {
        "id": "AwzFAONK7uQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_crit = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:32.681726Z",
          "iopub.execute_input": "2022-01-27T16:35:32.684191Z",
          "iopub.status.idle": "2022-01-27T16:35:32.691193Z",
          "shell.execute_reply.started": "2022-01-27T16:35:32.684151Z",
          "shell.execute_reply": "2022-01-27T16:35:32.690203Z"
        },
        "trusted": true,
        "id": "KBCZKvFC7uQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Let's train for 10 epochs"
      ],
      "metadata": {
        "id": "mJ9yYihu7uQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:32.696218Z",
          "iopub.execute_input": "2022-01-27T16:35:32.698783Z",
          "iopub.status.idle": "2022-01-27T16:35:32.778414Z",
          "shell.execute_reply.started": "2022-01-27T16:35:32.698743Z",
          "shell.execute_reply": "2022-01-27T16:35:32.77738Z"
        },
        "trusted": true,
        "id": "gtxjBWvo7uQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_split = 0.2\n",
        "\n",
        "train_data = FashionMNISTDataset('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\n",
        "\n",
        "# create 'lengths' for the split\n",
        "dataset_size = len(train_data)\n",
        "val_size = int(test_split * dataset_size)\n",
        "train_size = dataset_size - val_size\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(train_data, lengths = [train_size, val_size], generator=torch.Generator().manual_seed(42))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:32.779936Z",
          "iopub.execute_input": "2022-01-27T16:35:32.780605Z",
          "iopub.status.idle": "2022-01-27T16:35:37.192785Z",
          "shell.execute_reply.started": "2022-01-27T16:35:32.780567Z",
          "shell.execute_reply": "2022-01-27T16:35:37.191881Z"
        },
        "trusted": true,
        "id": "QAEGz1DQ7uQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(batch_size=10, GPU=False, epochs=10, lr=0.001):\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    train_loader = DataLoader(train_set,\n",
        "                              batch_size = batch_size,\n",
        "                              shuffle = True)\n",
        "    test_loader = DataLoader(val_set,\n",
        "                             batch_size = batch_size,\n",
        "                             shuffle = True)\n",
        "\n",
        "    model = myCNN()\n",
        "\n",
        "    if GPU:\n",
        "        model.to(device)\n",
        "\n",
        "    eval_crit = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    \"\"\"\n",
        "    Let's keep track of the training losses by appending them to a vector\n",
        "    At the end of training, we will plot the losses on the training set and on the test set for each epoch\n",
        "    This will help us see if there is a potential to improve results if we train for more epochs.\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    #Let's also keep track of the number of correct classifications in each epoch\n",
        "    train_correct = []\n",
        "    test_correct = []\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        train_correct_epoch = 0\n",
        "        test_correct_epoch = 0\n",
        "\n",
        "        for X_train, y_train in train_loader:\n",
        "            if GPU:\n",
        "                X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "\n",
        "            #apply our model\n",
        "            y_pred = model(X_train.float())\n",
        "\n",
        "            #compute the loss\n",
        "            loss = eval_crit(y_pred, y_train)\n",
        "\n",
        "            \"\"\"\n",
        "            Count the number of correctly predicted items:\n",
        "            y_pred contains the probability for the training examples to belong to each of the 10 possible classes\n",
        "            therefore, in order to get the predicted class for each example, we need to do this for each row:\n",
        "            find the maximum probabiliy and return its column number\n",
        "            \"\"\"\n",
        "            predicted_classes = torch.max(y_pred.data, 1)[1]\n",
        "            train_correct_epoch += (predicted_classes == y_train).sum() #add the number of correctly predicted items in this batch\n",
        "\n",
        "            #update the model parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_losses.append(loss)\n",
        "        train_correct.append(train_correct_epoch.item()*100/len(train_set))\n",
        "\n",
        "        #Evaluate the performance of the currect model on the test set\n",
        "        #we first set our model into eval mode\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for X_test, y_test in test_loader:\n",
        "                if GPU:\n",
        "                    X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "                #apply our model\n",
        "                y_pred = model(X_test.float())\n",
        "\n",
        "                predicted_classes = torch.max(y_pred.data, 1)[1]\n",
        "                test_correct_epoch += (predicted_classes == y_test).sum()\n",
        "\n",
        "            loss = eval_crit(y_pred, y_test)\n",
        "\n",
        "            test_losses.append(loss)\n",
        "            test_correct.append(test_correct_epoch.item()*100/len(val_set))\n",
        "\n",
        "            #print(f'\\t\\t [Eval] Loss: {loss:7.5} Accuracy: {test_correct[-1]:4.2f} ({test_correct_epoch} correct / {len(val_set)})')\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        print(f'Epoch: {i+1} [Train] Loss: {train_losses[-1]:7.5} Accuracy: {train_correct[-1]:4.2f} {train_correct_epoch} / {len(train_set)} [Eval] Loss: {test_losses[-1]:7.5} Accuracy: {test_correct[-1]:4.2f}')\n",
        "\n",
        "    print(f'Duration of training (min): {(time.time()-start)/60}')\n",
        "\n",
        "    result = (batch_size, train_correct, test_correct, train_losses, test_losses)\n",
        "\n",
        "    return model, result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:37.194178Z",
          "iopub.execute_input": "2022-01-27T16:35:37.194524Z",
          "iopub.status.idle": "2022-01-27T16:35:37.216616Z",
          "shell.execute_reply.started": "2022-01-27T16:35:37.194488Z",
          "shell.execute_reply": "2022-01-27T16:35:37.215824Z"
        },
        "trusted": true,
        "id": "IMFDS9vV7uQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model10, result10 = train_model(batch_size=10, GPU=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:35:37.217801Z",
          "iopub.execute_input": "2022-01-27T16:35:37.218245Z",
          "iopub.status.idle": "2022-01-27T16:38:04.896141Z",
          "shell.execute_reply.started": "2022-01-27T16:35:37.218205Z",
          "shell.execute_reply": "2022-01-27T16:38:04.895183Z"
        },
        "trusted": true,
        "id": "q2G0uTzP7uQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Plot the loss and accuracy during training"
      ],
      "metadata": {
        "id": "KQBmLO_t7uQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(res_arr):\n",
        "    plt.figure(figsize=(12,12))\n",
        "\n",
        "    for result in res_arr:\n",
        "        (bsz, train_acc, test_acc, train_loss, test_loss) = result\n",
        "\n",
        "        plt.subplot(211)\n",
        "        plt.plot(train_acc, label=f'training accuracy b={bsz}')\n",
        "        plt.plot(test_acc, label=f'eval set accuracy b={bsz}')\n",
        "        plt.title('Accuracy (%) at the end of each epoch')\n",
        "        plt.ylim(50,100)\n",
        "        plt.legend();\n",
        "\n",
        "        plt.subplot(212)\n",
        "        plt.plot(train_loss, label=f'training loss b={bsz}')\n",
        "        plt.plot(test_loss, label=f'eval set loss b={bsz}')\n",
        "        plt.title('Loss at the end of each epoch')\n",
        "        plt.ylim(0,5)\n",
        "        plt.legend();"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:38:04.898323Z",
          "iopub.execute_input": "2022-01-27T16:38:04.898897Z",
          "iopub.status.idle": "2022-01-27T16:38:04.908288Z",
          "shell.execute_reply.started": "2022-01-27T16:38:04.898853Z",
          "shell.execute_reply": "2022-01-27T16:38:04.9076Z"
        },
        "trusted": true,
        "id": "wNHRCs5c7uQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results([result10])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:38:04.911201Z",
          "iopub.execute_input": "2022-01-27T16:38:04.911479Z",
          "iopub.status.idle": "2022-01-27T16:38:05.20664Z",
          "shell.execute_reply.started": "2022-01-27T16:38:04.911453Z",
          "shell.execute_reply": "2022-01-27T16:38:05.205591Z"
        },
        "trusted": true,
        "id": "uXPi_zYh7uQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save this model for now."
      ],
      "metadata": {
        "id": "MIKJ4p117uQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'FashionMNIST-CNN-Model-Batch10.pt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:38:05.208378Z",
          "iopub.execute_input": "2022-01-27T16:38:05.208754Z",
          "iopub.status.idle": "2022-01-27T16:38:05.217316Z",
          "shell.execute_reply.started": "2022-01-27T16:38:05.208719Z",
          "shell.execute_reply": "2022-01-27T16:38:05.216433Z"
        },
        "trusted": true,
        "id": "UIPqOj1f7uQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Considerations on loss fluctuations\n",
        "\n",
        "Let's have a look at the first plot from section 4.2. Notice how the losses fluctuate between epochs during training ?   \n",
        "We are trying to train a network of ~ 60k parameters.  \n",
        "We are training it on 10k samples.   \n",
        "\n",
        "And we crunch training data in batches of 10. After each batch of 10, we evaluate the loss and update our parameters.   \n",
        "\n",
        "Maybe our batch size is too small or our learnin rate is too big; both can lead to high variations in the loss metric.  \n",
        "\n",
        "Let's look at the interraction of batch size and losses by using a batch_size of 64 and one of 4.\n",
        "\n"
      ],
      "metadata": {
        "id": "dVay4evS7uQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs = [4, 10, 32, 128, 256]\n",
        "\n",
        "models, results = [], []\n",
        "\n",
        "for b in bs:\n",
        "    if b == 10:\n",
        "        models.append(model10)\n",
        "        results.append(result10)\n",
        "        continue\n",
        "    model, result = train_model(batch_size=b, GPU=True, epochs=10)\n",
        "    models.append(model)\n",
        "    results.append(result)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:38:05.218931Z",
          "iopub.execute_input": "2022-01-27T16:38:05.219551Z",
          "iopub.status.idle": "2022-01-27T16:45:20.059672Z",
          "shell.execute_reply.started": "2022-01-27T16:38:05.219509Z",
          "shell.execute_reply": "2022-01-27T16:45:20.058946Z"
        },
        "trusted": true,
        "id": "14OuggHy7uQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(results)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T16:45:20.061229Z",
          "iopub.execute_input": "2022-01-27T16:45:20.061743Z",
          "iopub.status.idle": "2022-01-27T16:45:20.465409Z",
          "shell.execute_reply.started": "2022-01-27T16:45:20.061705Z",
          "shell.execute_reply": "2022-01-27T16:45:20.464492Z"
        },
        "trusted": true,
        "id": "rdIqmpB37uQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the best evaluation results happened for batch sizes 4 and 10.\n",
        "\n",
        "So I'll use batch size 10 model for a final evaluation on the unseen test data."
      ],
      "metadata": {
        "id": "qsP2IYPu7uQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = models[1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T17:04:24.718978Z",
          "iopub.execute_input": "2022-01-27T17:04:24.719346Z",
          "iopub.status.idle": "2022-01-27T17:04:24.726408Z",
          "shell.execute_reply.started": "2022-01-27T17:04:24.719316Z",
          "shell.execute_reply": "2022-01-27T17:04:24.725341Z"
        },
        "trusted": true,
        "id": "DUUZFKI57uQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. CNN model evaluation"
      ],
      "metadata": {
        "id": "HQBU0xVd7uQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set this to False if training was done on CPU\n",
        "GPU = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T17:04:26.275744Z",
          "iopub.execute_input": "2022-01-27T17:04:26.276056Z",
          "iopub.status.idle": "2022-01-27T17:04:26.27982Z",
          "shell.execute_reply.started": "2022-01-27T17:04:26.276027Z",
          "shell.execute_reply": "2022-01-27T17:04:26.278865Z"
        },
        "trusted": true,
        "id": "lADfoXSA7uQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.eval() #sets our model into evaluation mode <-- check PyTorch documentation\n",
        "with torch.no_grad(): #prevents adjusting the network parameters by mistake\n",
        "    corr_pred = 0\n",
        "    for X_test, y_test in test_loader:\n",
        "        if GPU:\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "        y_pred = final_model(X_test.float())\n",
        "        # for each test example, the prediction is a vector of 10 items. Item number k represents the probability of this\n",
        "        # example belonging to class k. To obtain the class label from this vector, we extract the position of the item\n",
        "        # with the highest value in the 10-item array.\n",
        "        labels_pred = torch.max(y_pred,1)[1] #get the labels for the predictions\n",
        "        corr_pred += (labels_pred == y_test).sum()\n",
        "\n",
        "print(f'Accuracy: {corr_pred.item()*100/(len(test_data))}%')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-27T17:04:27.388892Z",
          "iopub.execute_input": "2022-01-27T17:04:27.389208Z",
          "iopub.status.idle": "2022-01-27T17:04:28.125908Z",
          "shell.execute_reply.started": "2022-01-27T17:04:27.389179Z",
          "shell.execute_reply": "2022-01-27T17:04:28.125128Z"
        },
        "trusted": true,
        "id": "KLI60Oyq7uQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it. Model performance still has room for improvement. I can do some data augmentation (various transformations applied to the training data), but it's beyond the scope of this notebook. I'll leave achieving highest performance for another notebook."
      ],
      "metadata": {
        "id": "s_LwcJMf7uQh"
      }
    }
  ]
}